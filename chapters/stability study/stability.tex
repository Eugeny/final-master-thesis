
\subsection{Corruptions}  
    For practical reasons it is important to not only evaluate the models on high-quality data exclusively, but also to know how the predictions will degrade when the input's data quality decreases. Having a model for fluorescence \textit{in silico} labeling that can additionally alarm end users when the predictions should not be relied upon is very useful in practice. Although the DIC microscopy is a relatively easy technique, there are still setting up procedures taking place that can be prone to errors. Additionally, as the models are not easily generalizable across phenotypes as well as between fixed and not fixed cells, an alarming system that is able to catch these situations would be useful to save time and cost of lab work. In order to measure the stability or robustness of the models towards data degeneration they were evaluated on the corrupted or "bad" input DIC images. There are two sources of "bad" images that can be used for such estimations. The first are actually corrupted images made in the laboratory. Such corruptions may come from different sources: for example, an oil bubble landed on the microscope lenses, low density of the cell on the image, over- or underexposure during image acquisition. Another source of image corruption would be images with artificial or pseudocorruptions created manually via image processing. They allow more systematic investigation of the impact of a corruption effect. Artificial corruptions allow to vary the severity of the corruption keeping the original fluorescence data intact.
    
    %This chapter first provides a description of artificial corruptions used to evaluate previously trained models on. Afterwards the real examples of corruptions acquired from the lab are 
    \subsubsection{Artificial corruptions}
        \input{content/stability/artificial corruptions.tex}
    \subsubsection{Real corruptions}
        \label{section:real-corruptions}
        \input{content/stability/real corruptions.tex}
    \subsubsection{Improving predictions with additional corruption augmentations}
        \label{section:augments-againts-corruptions}
        \input{content/stability/augmentations.tex}
    \subsubsection{Generalizability across phenotypes}
        TODO train the model on one phenotype and predict on the other, compare predictions (visually?)
        postprocessing with metrics then?