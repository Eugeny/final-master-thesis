\section{Drift Detection}
\subsection{Embeddings visualization}
Emeddings of the UNet represent themselves a highly compressed and dense information in comparison to a UNet input - an image itself. Visual information (images) is extremely redundant. Having a value of one pixel it is easy to predict the neighboring ones. UNet however compresses this information into a smaller-dimensions representation. In the architecture of the UNet presented in Figure TODO the size of the embeddings in the bottleneck layer of the network is \textit{256x16x16}, meaning that there are 256 filters were applied to a previous layer and the image was compressed from the resilution of \textit{256x256} to \textit{16x16}, meaning that the image was essitially compressed by the factor of \textit{16x16 = 256}. This compression is done by the \textit{bottleneck} layer.

One might wonder why the information from the small objects like Golgi for example won't be lost with such a compression. The answer for this is a receptive field of the network. Receptive field is the region in the input that produces the feature in the hidden layer (cite https://distill.pub/2019/computing-receptive-fields/ Andre Araujo). In the architecture presented the compression happens due to the \textit{MaxPool} layers with a kernel size of \textit{2x2} with a stride of $2$. After the first \textit{MaxPool} layer one pixel in the embedding will correspond to \textit{2x2=4} pixels of the input. However already after the second layer of compression one pixel in the embedding will contain \textit{4x4 = 16} pixels of the input. This is how the receptive field of the network increases with the compression while the redundancy of the input information decreases.

To visualize the embedding of the UNet one has to first flatten the bottleneck layer into a vector, in the current network implementation the size of such vector will be \textit{655536} which too high dimesional. In order to comprehend the embeddings better one has to first perfom any dimensionaluty reduction algorithm. One of the options is to compress the vector to still a somewhat high-dimentional vector, however with a fewer dimention, or to compress it up to 2D or 3D-dimentional representation, which can be easily comprehendable by humans. Both compessions are presented in this section.

\subsection{Drift detection vs Outliers detection}

After the developement phase or a training of the model is finished, it will be moved into a deployment or a production, where it is supposed to maintain an expected quality of predictions. However input data is not always a stable source, therefore one should constantly maintain the quality of predictions and do a regular check-ups for outliers as well as to alert the drift of the data early enough. Drift detection happens on the raw data in absence of the ground truth labels and serves as a signal that the input data differs a lot from the training data.

There is a significant difference between distinguishing drift of the whole source of data in comparison to detecting single outliers. When one talks about a drift detection, one looks at the whole new input data as a distribution and checks if there is a significant shift in comparison to the data used during training.

There are two possible reactions after the drift is detected, one alerts the user that the predictions became unreliable, and therefore one should consider expanding the dataset by adding the labeled data from the drifted distribution to include it in training or apply some different logic on the model outputs. Although when an outlier is detected, the model might request a human assistant for some particular input, because this input is too unfamiliar to the model and it possible won't give a good prediction on this one.

The goal of outliers detection is to decide on the single instances whether or not they are different from training on unusual in some wy or another. They of course might appear both in training and predictions.

Data drift and outlier detection can co-exist. It might be that the input is drifted, but there are no outliers, it might be that there are a lot of outliers, but the data was not drifted. (cite https://towardsdatascience.com/what-is-the-difference-between-outlier-detection-and-data-drift-detection-534b903056d4) But during the production the good approach is to monitor both.

Important observation here is that the drift detector should be robust to outliers. The system should not send an alert as soon as it sees a suspicious sample due to the fact that outlier might be present in the original data distribution as well. But the alert should happen when there are many such samples. To compare original training data distribution and the new one from inputs different statistical tests like Kolmogorov-Smirnov, Chi-squared and etc. can used.

The need of maintaining drift detection or outlier detection depends on the costs on the errors. If the cost of a single error is too high, one should use an outlier detection, but when one needs a test to decide when to label new data - drift detection would be a better approach.

In summary, the drift detection is needed only when the meaningful shifts of the input data distribution from the training distribution need to be detected, whereas the outlier detector aims at finding unusual single instances uin the inputs.

\subsection{Drift Detection}
Assume that during training labeled data comes from a distribution $p$, meaning $\{(x_1, y_1), ..., (x_n, y_n)\} \sim p$ and syring deployment unlabeled data comes from a distribution $q$, meaning $\{x_1', ..., x_m'\} \sim q$. The goal of the drift detection is to determine if $q(x')$ is the same data distribution as $p(x)$. Or if to put it more formally $H_0:p(x) = q(x)$ and $H_A:p(x) \neq q(x)$