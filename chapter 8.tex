\section{Drift Detection}
\subsection{Embeddings visualization}
Emeddings of the UNet represent themselves a highly compressed and dense information in comparison to a UNet input - an image itself. Visual information (images) is extremely redundant. Having a value of one pixel it is easy to predict the neighboring ones. UNet however compresses this information into a smaller-dimensions representation. In the architecture of the UNet presented in Figure TODO the size of the embeddings in the bottleneck layer of the network is \textit{256x16x16}, meaning that there are 256 filters were applied to a previous layer and the image was compressed from the resilution of \textit{256x256} to \textit{16x16}, meaning that the image was essitially compressed by the factor of \textit{16x16 = 256}. This compression is done by the \textit{bottleneck} layer.

One might wonder why the information from the small objects like Golgi for example won't be lost with such a compression. The answer for this is a receptive field of the network. Receptive field is the region in the input that produces the feature in the hidden layer (cite https://distill.pub/2019/computing-receptive-fields/ Andre Araujo). In the architecture presented the compression happens due to the \textit{MaxPool} layers with a kernel size of \textit{2x2} with a stride of $2$. After the first \textit{MaxPool} layer one pixel in the embedding will correspond to \textit{2x2=4} pixels of the input. However already after the second layer of compression one pixel in the embedding will contain \textit{4x4 = 16} pixels of the input. This is how the receptive field of the network increases with the compression while the redundancy of the input information decreases.

To visualize the embedding of the UNet one has to first flatten the bottleneck layer into a vector, in the current network implementation the size of such vector will be \textit{655536} which too high dimesional. In order to comprehend the embeddings better one has to first perfom any dimensionaluty reduction algorithm. One of the options is to compress the vector to still a somewhat high-dimentional vector, however with a fewer dimention, or to compress it up to 2D or 3D-dimentional representation, which can be easily comprehendable by humans. Both compessions are presented in this section.

\subsection{Drift detection vs Outliers detection}

After the developement phase or a training of the model is finished, it will be moved into a deployment or a production, where it is supposed to maintain an expected quality of predictions. However input data is not always a stable source, therefore one should constantly maintain the quality of predictions and do a regular check-ups for outliers as well as to alert the drift of the data early enough. Drift detection happens on the raw data in absence of the ground truth labels and serves as a signal that the input data differs a lot from the training data.

There is a significant difference between distinguishing drift of the whole source of data in comparison to detecting single outliers. When one talks about a drift detection, one looks at the whole new input data as a distribution and checks if there is a significant shift in comparison to the data used during training.

There are two possible reactions after the drift is detected, one alerts the user that the predictions became unreliable, and therefore one should consider expanding the dataset by adding the labeled data from the drifted distribution to include it in training or apply some different logic on the model outputs. Although when an outlier is detected, the model might request a human assistant for some particular input, because this input is too unfamiliar to the model and it possible won't give a good prediction on this one.

The goal of outliers detection is to decide on the single instances whether or not they are different from training on unusual in some wy or another. They of course might appear both in training and predictions.

Data drift and outlier detection can co-exist. It might be that the input is drifted, but there are no outliers, it might be that there are a lot of outliers, but the data was not drifted. (cite https://towardsdatascience.com/what-is-the-difference-between-outlier-detection-and-data-drift-detection-534b903056d4) But during the production the good approach is to monitor both.

Important observation here is that the drift detector should be robust to outliers. The system should not send an alert as soon as it sees a suspicious sample due to the fact that outlier might be present in the original data distribution as well. But the alert should happen when there are many such samples. To compare original training data distribution and the new one from inputs different statistical tests like Kolmogorov-Smirnov, Chi-squared and etc. can used.

The need of maintaining drift detection or outlier detection depends on the costs on the errors. If the cost of a single error is too high, one should use an outlier detection, but when one needs a test to decide when to label new data - drift detection would be a better approach.

In summary, the drift detection is needed only when the meaningful shifts of the input data distribution from the training distribution need to be detected, whereas the outlier detector aims at finding unusual single instances uin the inputs.

\subsection{Drift Detection}
Assume that during training labeled data comes from a distribution $p$, meaning $\{(x_1, y_1), ..., (x_n, y_n)\} \sim p$ and syring deployment unlabeled data comes from a distribution $q$, meaning $\{x_1', ..., x_m'\} \sim q$. The goal of the drift detection is to determine if $q(x')$ is the same data distribution as $p(x)$. Or if to put it more formally $H_0:p(x) = q(x)$ and $H_A:p(x) \neq q(x)$

Having the samples from both distributions or the representation of these samples in lower dimension, one can then choose a statistical hypothesis test to compare these distributions. 

cite https://arxiv.org/pdf/1605.09522.pdf

\subsubsection{Maximum Mean Discrepancy}

The test for determinig wether two previously mentioned distributions are the same that was used in this work is one of the multivariate kernel two-sample tests: Maximum Mean Discrepancy or shotly MMD. The idea behid any two-sample testing is to choose two random samples, where each was taken from one of the two different distributions and afterwards to decide wether the difference in them is statistically significant. 

MMD is a kernel-based technique and it allows to distinguish between two distributions based on their mean embeddings (cite and rephrase https://arxiv.org/pdf/1810.11953.pdf) in a reproducing kernel Hilbert space (RKHS).

The idea behind a Hilbert space embedding distribution (or a kernel mean embedding) is to map a distribution into a point in a reproducing Hilbert space. By doing this all powerful kernel methods are allowed to be used for probability measures, resulting in methods like kernel two-sample testing for instance. One of the widely known kernel methods if a support vector machines (SVM).

To understand why kernel mean embeddings are so successful one has to first understand what a kernel function is. With
the help of kernel functions an inner product of elements $x, y \in \mathcal{X}$ in some high-dimensional feature space can be calculated. From cite Smola 2002 if the kernel function if positive definite, then there always exists a dot product space $\mathscr{H}$ as well there exists a function that maps a space from which theses elements are coming from into a mentioned high-dimensional space: $\phi : \mathcal{X} \rightarrow \mathscr{H}$ such that $k(x, y) = {\langle\phi(x), \phi(y)\rangle}_{\mathscr{H}}$ and most importantly there is no need for explicit computation of $\phi$. Therefore if there exists an algorithm that can be expressed through the dot product of $\langle x, y \rangle$ cite Sch√∂lkopf Smola 1998 then the kernel function can be applied to this dot product and this is called a \textit{kernel trick}.

Now kernel mean embedding actually extends the above mentioned feature map $\phi$ to the space of probability distributions. In this space each probability distribution will be mapped to a mean function defined as follows:

\begin{equation}
    \phi(\mathds{P}) = \mu_{\mathds{P}} := \int_{\mathcal{X}}k(x, \cdot)d\mathds{P}(x)
\end{equation}

Here $k(x, \cdot)$ is a positive definite symmetric kernel function. Generally the main goal is to map a distribution $\mathds{P}$ to an point in the feature space $\mathscr{H}$ and this feature space is exactly an RKHS that corresponds to a kernel $k$. Such a mapping might be useful because it captures all information about the initial distribution $\mathds{P}$. This mapping $\mathds{P} \rightarrow \mu_\mathds{P}$ is injective. This means that $||\mu_\mathds{P} - \mu_\mathds{Q}||_{\mathscr{H}} = 0$ if and only if $\mathds{P} = \mathds{Q}$, which means here that $\mathds{P}$ and $\mathds{Q}$ is the same distribution. Additionally the fact that the mapping is injective makes it possible to use such characterization of a distribution to be used in two-sample homogenneity tests, which is exactly what is needed here. 

To estimate a the kernel mean embedding is much easier than to estimate the distribution itself, this is successfully used in data-generating processes as well as it improves some statistical inference methods like two-sample testin again.  

RKHS methods now can be applied to probability measures, which results in finding out many useful applications one of which is kernel two-sample testing. 