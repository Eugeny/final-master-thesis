\section{Drift Detection}
\subsection{Embeddings visualization}
Emeddings of the UNet represent themselves a highly compressed and dense information in comparison to a UNet input - an image itself. Visual information (images) is extremely redundant. Having a value of one pixel it is easy to predict the neighboring ones. UNet however compresses this information into a smaller-dimensions representation. In the architecture of the UNet presented in Figure TODO the size of the embeddings in the bottleneck layer of the network is \textit{256x16x16}, meaning that there are 256 filters were applied to a previous layer and the image was compressed from the resilution of \textit{256x256} to \textit{16x16}, meaning that the image was essitially compressed by the factor of \textit{16x16 = 256}. This compression is done by the \textit{bottleneck} layer.

One might wonder why the information from the small objects like Golgi for example won't be lost with such a compression. The answer for this is a receptive field of the network. Receptive field is the region in the input that produces the feature in the hidden layer (cite https://distill.pub/2019/computing-receptive-fields/ Andre Araujo). In the architecture presented the compression happens due to the \textit{MaxPool} layers with a kernel size of \textit{2x2} with a stride of $2$. After the first \textit{MaxPool} layer one pixel in the embedding will correspond to \textit{2x2=4} pixels of the input. However already after the second layer of compression one pixel in the embedding will contain \textit{4x4 = 16} pixels of the input. This is how the receptive field of the network increases with the compression while the redundancy of the input information decreases.

To visualize the embedding of the UNet one has to first flatten the bottleneck layer into a vector, in the current network implementation the size of such vector will be \textit{655536} which too high dimesional. In order to comprehend the embeddings better one has to first perfom any dimensionaluty reduction algorithm. One of the options is to compress the vector to still a somewhat high-dimentional vector, however with a fewer dimention, or to compress it up to 2D or 3D-dimentional representation, which can be easily comprehendable by humans. Both compessions are presented in this section.

\subsection{Drift detection vs Outliers detection}

After the developement phase or a training of the model is finished, it will be moved into a deployment or a production, where it is supposed to maintain an expected quality of predictions. However input data is not always a stable source, therefore one should constantly maintain the quality of predictions and do a regular check-ups for outliers as well as to alert the drift of the data early enough. Drift detection happens on the raw data in absence of the ground truth labels and serves as a signal that the input data differs a lot from the training data.

There is a significant difference between distinguishing drift of the whole source of data in comparison to detecting single outliers. When one talks about a drift detection, one looks at the whole new input data as a distribution and checks if there is a significant shift in comparison to the data used during training.

There are two possible reactions after the drift is detected, one alerts the user that the predictions became unreliable, and therefore one should consider expanding the dataset by adding the labeled data from the drifted distribution to include it in training or apply some different logic on the model outputs. Although when an outlier is detected, the model might request a human assistant for some particular input, because this input is too unfamiliar to the model and it possible won't give a good prediction on this one.

The goal of outliers detection is to decide on the single instances whether or not they are different from training on unusual in some wy or another. They of course might appear both in training and predictions.

Data drift and outlier detection can co-exist. It might be that the input is drifted, but there are no outliers, it might be that there are a lot of outliers, but the data was not drifted. (cite https://towardsdatascience.com/what-is-the-difference-between-outlier-detection-and-data-drift-detection-534b903056d4) But during the production the good approach is to monitor both.

Important observation here is that the drift detector should be robust to outliers. The system should not send an alert as soon as it sees a suspicious sample due to the fact that outlier might be present in the original data distribution as well. But the alert should happen when there are many such samples. To compare original training data distribution and the new one from inputs different statistical tests like Kolmogorov-Smirnov, Chi-squared and etc. can used.

The need of maintaining drift detection or outlier detection depends on the costs on the errors. If the cost of a single error is too high, one should use an outlier detection, but when one needs a test to decide when to label new data - drift detection would be a better approach.

In summary, the drift detection is needed only when the meaningful shifts of the input data distribution from the training distribution need to be detected, whereas the outlier detector aims at finding unusual single instances uin the inputs.

\subsection{Drift Detection}
Assume that during training labeled data comes from a distribution $p$, meaning $\{(x_1, y_1), ..., (x_n, y_n)\} \sim p$ and syring deployment unlabeled data comes from a distribution $q$, meaning $\{x_1', ..., x_m'\} \sim q$. The goal of the drift detection is to determine if $q(x')$ is the same data distribution as $p(x)$. Or if to put it more formally $H_0:p(x) = q(x)$ and $H_A:p(x) \neq q(x)$

Having the samples from both distributions or the representation of these samples in lower dimension, one can then choose a statistical hypothesis test to compare these distributions. 

cite https://arxiv.org/pdf/1605.09522.pdf

\subsubsection{Kernel methods and two-sample testing}

The test for determinig wether two previously mentioned distributions are the same that was used in this work is one of the multivariate kernel two-sample tests: Maximum Mean Discrepancy or shotly MMD. The idea behid any two-sample testing is to choose two random samples, where each was taken from one of the two different distributions and afterwards to decide wether the difference in them is statistically significant. 

MMD is a kernel-based technique and it allows to distinguish between two distributions based on their mean embeddings (cite and rephrase https://arxiv.org/pdf/1810.11953.pdf) in a reproducing kernel Hilbert space (RKHS).

The idea behind a Hilbert space embedding distribution (or a kernel mean embedding) is to map a distribution into a point in a reproducing Hilbert space. By doing this all powerful kernel methods are allowed to be used for probability measures, resulting in methods like kernel two-sample testing for instance. One of the widely known kernel methods if a support vector machines (SVM).

To understand why kernel mean embeddings are so successful one has to first understand what a kernel function is. With
the help of kernel functions an inner product of elements $x, y \in \mathcal{X}$ in some high-dimensional feature space can be calculated. From cite Smola 2002 if the kernel function if positive definite, then there always exists a dot product space $\mathscr{H}$ as well there exists a function that maps a space from which theses elements are coming from into a mentioned high-dimensional space: $\phi : \mathcal{X} \rightarrow \mathscr{H}$ such that $k(x, y) = {\langle\phi(x), \phi(y)\rangle}_{\mathscr{H}}$ and most importantly there is no need for explicit computation of $\phi$. Therefore if there exists an algorithm that can be expressed through the dot product of $\langle x, y \rangle$ cite Sch√∂lkopf Smola 1998 then the kernel function can be applied to this dot product and this is called a \textit{kernel trick}.

Now kernel mean embedding actually extends the above mentioned feature map $\phi$ to the space of probability distributions. In this space each probability distribution will be mapped to a mean function defined as follows:

\begin{equation}
    \phi(\mathds{P}) = \mu_{\mathds{P}} := \int_{\mathcal{X}}k(x, \cdot)d\mathds{P}(x)
\end{equation}

Here $k(x, \cdot)$ is a positive definite symmetric kernel function. Generally the main goal is to map a distribution $\mathds{P}$ to an point in the feature space $\mathscr{H}$ and this feature space is exactly an RKHS that corresponds to a kernel $k$. Such a mapping might be useful because it captures all information about the initial distribution $\mathds{P}$. This mapping $\mathds{P} \rightarrow \mu_\mathds{P}$ is injective. This means that $||\mu_\mathds{P} - \mu_\mathds{Q}||_{\mathscr{H}} = 0$ if and only if $\mathds{P} = \mathds{Q}$, which means here that $\mathds{P}$ and $\mathds{Q}$ is the same distribution. Additionally the fact that the mapping is injective makes it possible to use such characterization of a distribution to be used in two-sample homogenneity tests, which is exactly what is needed here. 

To estimate a the kernel mean embedding is much easier than to estimate the distribution itself, this is successfully used in data-generating processes as well as it improves some statistical inference methods like two-sample testing again. It is also a helpful approach when instead of a data points for examples of testing and training datasets there are probability distributions. 

Inner product $\langle x, y \rangle$ can be viewed as a similarity measure between $x$ and $y$. This inner product incudes a class of linear functions and this class is too restrictive for many applications, however there is a simple possible extension to add non-linearities to it with the mapping:

\begin{equation}
    \phi: \mathcal{X} \rightarrow \mathcal{F}
\end{equation}

where

\begin{equation}
    \phi: x \rightarrow \phi(x)
    \label{equation:positive-definite}
\end{equation}

Here $\mathcal{F}$ is high-dimensional feature space and it is possible to evaluate then:

\begin{equation}
    k(x, y) := {\langle\phi(x), \phi(y)\rangle}_{\mathcal{F}}
\end{equation}

with 

${\langle \cdot, \cdot \rangle}_{\mathcal{F}}$ denoting an innter product in of $\mathcal{F}$.

Now $ k(x, y)$ is already a non-linear similarity measure between $x$ and $y$. Now to get a non-linear version of the algorithms that use dot product, $\langle x, y\rangle$ can be simply sustituted with $ {\langle\phi(x), \phi(y)\rangle}_{\mathcal{F}}$. 

Let's define the following mapping that represents in $\mathcal{X}$ any probability measure $\mathds{P}$ and denote it as $\mu_{\mathds{P}}$:


This mapping is called a kernel mean embedding.

\begin{definition}[Kernel mean embedding]
    cite Berlinet and Thomas Agnan 2004
    The kernel mean embedding of probability measure in $M^1_+(\mathcal{X})$ into RKHS $\mathscr{H}$ endowed with a reproducing kernel $k: \mathscr{H} \times \mathscr{H} \rightarrow \mathds{R}$ is defined by a mapping 
    \begin{equation}
        \mu : M^1_+(\mathcal{X}) \rightarrow \mathscr{H}, \mathds{P} \rightarrow \int k(x, \cdot)d\mathds{P}(x)
    \end{equation}
\end{definition}

However usually there is no access to the distribution $\mathds{P}$ and that is why one cannot directly compute $\mu_{\mathds{P}}$. Fortuntely there are samples that can be drawn from this distribution and with their use one can make a good approximation of $\hat{\mu}_{\mathds{P}}$ of the tru kernel mean embedding $\mu_{\mathds{P}}$. One of such approximations could be the following unbiased estimate:
\begin{equation}
    \hat{\mu}_{\mathds{P}} := \frac{1}{n} \sum_{i=1}^n k(x_i, \cdot)
\end{equation}

Moreover this estimator $\hat{\mu}_{\mathds{P}}$  will converge to $\mu_{\mathds{P}}$ by the law of large numbers as $n \rightarrow \inf$

\begin{definition}[Characteristic kernel]
    A kernel $k$ is a characteristic kernel if the map $\mu: \mathds{P} \rightarrow \mu_{\mathds{P}}$ is injective. If the reproducing kernel of the RKHS $\mathscr{H}$ is characteristic, then RKHS is called characteristic as well. 
\end{definition}

In machine learning applications and statistics the kernel mean embedding is as a metric for the probability distributions. And mean embeddings metric is actually just a specific case of a more general so-called integral probability metric (IPM) (M√ºller 1997). 

\begin{definition}[IPM]
    Let $\mathds{P}$ and  $\mathds{Q}$ be two probability measures on some measurable space $\mathcal{X}$. Then IPM is defined as follows:
    \begin{equation}
        \gamma [\mathcal{F}, \mathds{P}, \mathds{Q}] = \sup_{f\in \mathcal{F}} \left\{ \int f(x) d\mathds{P}(x) -  \int f(y) d\mathds{Q}(y) \right\}
    \end{equation}
    with $\mathcal{F}$ being a space of real-value bounded functions.
    \label{eq:ipm}
\end{definition}

\subsubsection{Maximum Mean Discrepancy}

Let's assume that $\mathcal{F} := \left\{f \;\middle| \;{\lVert f \rVert}_{\mathscr{H}} \leq 1\right\}$, it means that the supremum in Definition \ref{eq:ipm} is taken over functions in the unit ball in RKHS, then the mean embedding metric will be called \textit{maximum mean discrepancy} (MMD). 

\begin{definition}[IPM]
    Maimum mean discrepancy is defined as a distance between two mean embeddings of distributions:
    \begin{align}
        \begin{split}
            \textrm{MMD} [\mathscr{H}, \mathds{P}, \mathds{Q}] & = \sup_{ \lVert f \rVert \leq 1} \left\{ \int f(x) d\mathds{P}(x) -  \int f(y) d\mathds{Q}(y) \right\} \\ 
            & = \sup_{ \lVert f \rVert \leq 1} \left\{ \langle f, \mu_{\mathds{P}} - \mu_{\mathds{Q}} \rangle_{\mathscr{H}} \right\} \\
            & = {\lVert \mu_{\mathds{P}} - \mu_{\mathds{Q}} \rVert }_{\mathscr{H}}
        \end{split}
    \end{align}
    \label{eq:mmd}
\end{definition}

From \ref{eq:mmd} if $\mathscr{H}$ is characteristic follows that $\textrm{MMD}[\mathscr{H}, \mathds{P}, \mathds{Q}] = 0$ if and only if $\mathds{P} = \mathds{Q}$.

Similarily to the estimation of the kernel mean embedding through the samples drawn from a distribution, a biased empricial estimator of MMD can be obtained. 

Assume that there are two sets of samples $X=\{x_1, ..., x_n\}$ and $Y=\{y_1, ..., y_m\}$ drawn from two distributions $\mathds{P}$ and $\mathds{Q}$ corrspondingly. Then the biased estimate of MMD can be calculated as follows:

\begin{equation}
    \widehat{\textrm{MMD}}_b^2 [\mathscr{H}, X, Y] := \sup_{ {\lVert f \rVert}_{\mathscr{H}} \leq 1} \left\{ \frac{1}{n} \sum^n_{i=1}{f(x_i)} - \frac{1}{m} \sum^m_{j=1}{f(y_j)} \right\} 
\end{equation}

By replacing $\frac{1}{n}\sum^n_{i=1}{f(x_i)}$ with the empirical estimators of mean embeddings one would get:

\begin{equation}
    \widehat{\textrm{MMD}}_b^2 [\mathscr{H}, X, Y] = {\lVert \widehat{\mu}_{\mathds{P}} - \widehat{\mu}_{\mathds{Q}} \rVert }_{\mathscr{H}}^2
\end{equation}

However if an unbiased estimator of MMD through the kernel function $k$ is needed then one could use the following estimator from cite Borgwardt et al 2006 Corollary 2.3

\begin{align}
    \begin{split}
        \widehat{\textrm{MMD}}_b^2 [\mathscr{H}, X, Y] = & \frac{1}{n(n-1)} \sum^n_{i=1}\sum^n_{j \neq i}k(x_i, x_j)\\
        & + \frac{1}{m(m-1)} \sum^m_{i=1}\sum^n_{j \neq i}k(y_i, y_j) \\
        & -\frac{2}{nm} \sum^n_{i=1}\sum^m_{j=1}k(x_i, y_j)
    \end{split}
\end{align}

The most common application of MMD in statistics is a two-sample testing. Particularily, in testing the null hypothesis $H_0:{\lVert \widehat{\mu}_{\mathds{P}} - \widehat{\mu}_{\mathds{Q}} \rVert }_{\mathscr{H}}=0$ that two samples come from the same distribution against an alternative hypothesis $H_1:{\lVert \widehat{\mu}_{\mathds{P}} - \widehat{\mu}_{\mathds{Q}} \rVert }_{\mathscr{H}}\neq0$. But one has to be cautios here, even when both samples came from the same distribution it still might be that the MMD will be not zero due to the fact that this is an etimate and not a precise value and we have a finite number of smaples for this estimate.


RKHS is defined by a positive definite kernel in (\ref{equation:positive-definite})
RKHS methods now can be applied to probability measures, which results in finding out many useful applications one of which is kernel two-sample testing. Such a kernel is then also called a reproducing kernel. 

\subsection{Practical application of MMD}
The drift detection of corrupted samples for the problem of the thesis has been performed using Alibi DETECT open source library, tha focuses specifically on outlier, adversarial and drift detection algorithms (cite https://github.com/SeldonIO/alibi-detect). Which implements statistical hypothesis testing algorithms for detecting drifts in data. 

In works in the following way, before observing some data, one can specisfy the null hypothesis $H_0$ and the alternative hypothesis $H_1$ about the generating process behind the data (its distribution for ex.) and also specofy the test statistics $S(X)$ that one expects to be small under the hypothesis $H_0$ and large under the hypothesis $H_1$. Then during the observation of the new data after computing the the value of this test statistic $S(X)$, one computes $p = P(S(X)|H_0)$ which is called p-value - a probability that such an extreme value of the test statistic could habe been observed under the null hypothesis. If this probability if below the threshold $t$, then the assume that this is a drifted data, and not otherwise. So low p-value refuses the null-hypothesis. 


\subsubsection{Online MMD}


More specific an online MMD detector was used for etimating wether the new samples are drifted from the training distribution. The radial basis kernel function (Equation \ref{eq:radial-basis-kernel}) is used there by default, however there are also options to use any kernel function of preference.

\begin{equation}
    \label{eq:radial-basis-kernel}
    k(x, y) = exp\left(-\frac{{\lVert x - y \rVert}^2}{2\sigma^2}\right)
\end{equation}

One os the charateristics of an online drift detector is that it assumes that there is a big dataset of a reference data, that can be used to training as an example of a "correct" distribution. Online algorithms focus on single input at a time during its run. This single input would be sent into a test window where two-sample test-statistics (MMD essentially in this case) will be calculated. As soons as the test-statistic exceeds some pre-defined threshold the drift alert is send to a user. Apart from the threshold one has to define a so-called expected run-time (ERT). This time states how many inputs the detector should process on average before it makes a detection (false positive or true positive depending on which distributions the inputs were taken from). Another hyper parameter here is hidden in a size of a test-window. Becase the larger the window is, the more chance is there to detect a very slight drift, however with a smaller window one gets a much faster response to severe drift. 

It is usually recommened to reduce the dimensionality of the data before feeding it into the algorithm. However in this case the performance was exceptional even for high-dimensional embeddings of the UNET model. However there are also options on how to reduce the dimensionality of the data from the authors of the framework. They recommend even to use an untrained AutoEncoder (UAE), as even an untrained one contains some relatively good information about the global structure of the data. (cite someone?)