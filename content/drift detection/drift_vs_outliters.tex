Following the development phase, when the model training is finished, the model will be moved into deployment or production, where it is supposed to maintain an expected quality of predictions. However input data is not always a stable source of input. One should constantly maintain quality of predictions and do regular check-ups for outliers as well as to alert the end user about a drift in input data. Drift detection happens on raw data in absence of the ground truth labels and serves as a signal that the input data differs a lot from the data used for training, meaning that predictions became unreliable.

There is a significant difference between distinguishing drift of the whole source of data in comparison to detecting single outliers. In drift detection, one looks at the whole new input data as a distribution and checks if there is a significant shift in comparison to the data used during training.

There are two possible reactions after the drift is detected: alert the user that predictions became unreliable, and and therefore the expansion of the dataset should be considered by adding more labeled data from a newly drifted distribution in training, or applying some different logic on the model outputs. When an outlier is detected, a model might request human assistance for some particular input, because this input is too unfamiliar to the model and possibly it will not return good predictions on this one.

The goal of outlier detection is to decide on the single instances whether or not it is different from training data or unusual in one way or another. Outliers might appear in both training and predictions datasets.

Data drift and outlier detection can co-exist. It might be that the input has drifted, but there are no outliers, it might be that there are a lot of outliers, but the data was not drifted. (\cite{samuylova_2021}).

The key observation here is that the drift detector should be robust to outliers. The system should not send an alert as soon as it sees a suspicious sample due to the fact that outliers might be present in the original data distribution as well. But the alert should happen when there are many such samples. To compare original training data distribution with the new one from inputs different statistical tests like Kolmogorov-Smirnov, Chi-squared and others can used.

The need for maintaining drift detection or outlier detection depends on the cost of errors occurring. If the cost of a single error is too high, one should use an outlier detection, but when one needs a test to decide when to label new data - drift detection would be a better approach.

In summary, the drift detection is needed only when the meaningful shifts of the input data distribution from the training distribution need to be detected, whereas the outlier detector aims at finding unusual single instances in inputs. Here this is exactly the case, we train models assuming the correct setup of microscopy image acquisition, however changes in exposure, illumination, cell fixation procedure might alter DIC imaging. In this case the user has to be informed about it and choose afterwards whether more data should be added to the training set or whether the mode's predictions should not be used.
