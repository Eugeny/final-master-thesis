% TODO add overfitting image?
Regularization is mostly used to prevent the deep learning model to overfit on the training data and to be able to generalize well. The overfitting has happened to the models used in this research and therefore it is improtant to understant the tenchniques that can be used against it. There are are several approaches to regularize the model and they will be explained below.

\begin{itemize}
	\item Early-stopping

	Overfitting can be detected via visualizing train and validation losses. Training behaviour at first will be the usual one, meaning that both train and validation loss are gradually decreasing, however at some point the train loss continues to decrease, whereas the validation loss suddenly starts to increase (see Figure \ref{fig:actin-overfit}). Since the model haven't seen any of the data from the validation set, it means that it losses its capability to generalize on the unsees data, while improving its perfomance on the seen data (train set). This doesn't happen during earlier epochs. Assume that during training the model learns a complex decision surface, with the correct weights initialization the weights of the model will be quite small and random and therefore the best decision surface during the first epochs would be a smooth one. But during the later ones the difference in values of the weights grows and they become not similar anymore which also means that the decision surface becomes more complex and the model is now able to fit not only the training data itself, but also its noise [cite p111 Mitchell Machine Learning 1997]. And that is why stopping before the model became too complex, meaning to stop before the overfitting point, mitigates this problem.

	\item \emph{L1}- \emph{L2}-regularization

	The complexity of the deep model grows with the number of features it uses, sometimes the model may pay attention to the features that are not important to the outcome, or even considers a noise to be a feature. To prevent this one should decrease the weights associated with useless features, however one cannot know ahead which of them should be ignored, therefore one may limit them all [cite Ying 2019]. In order to do that, a penalty term in loss function is added:

	\begin{equation}
	\tilde{L}(Y, M(X, \theta)) = L(Y, M(X, \theta)) + \lambda R(\theta)
	\end{equation}

	for some $\lambda > 0$. This is called a \emph{soft-constrait} optimization. When $R(\theta)$ is of the form $R(\theta) = ||\theta||^2_2 = \sqrt{\sum\limits_i \theta_i^2}$ this is called \emph{L2}-regularization. When it is of form $R(\theta) = ||\theta||_1 = \sum\limits_i |\theta_i|$ this is called \emph{L1}-regularization. \emph{L2}-regularization used in combination with backpropagation is equivalent to weight decay. Weight decay is defined by [cite Hanson and Pratt 1988] as follows:
	\begin{equation}
		\theta_{t+1} = (1 - \lambda)\theta_t - \alpha \frac{\partial L}{\partial \theta_t}
	\end{equation}

	where $\alpha$ is a learning rate. Weight decay successfully affects more those weights the gradient change along which is smaller [cite DL-book p229]. \emph{L1}-regularization induces sparsity of the weights by assining some of them to zero, this could be also considered as a feature selection approach.

	\item Regularization layers

	Batch normalization and dropout layers are also considered to be a form of regularization. 

	\item Network reduction
	
	Since learning a too complex and noise-fitting decision surface might be an often a cause of an overfit, another way to mitigate it would to be reduce the space of the possible decision surfaces and therefore make the surface simpler so that it cannot fit into the noise from the data. By changing the number of adaptive parameters in the network, the complexity can be varied [cite Page 332 Bishop book].

	\item Expansion of the training data

	For a successfull training a model needs to have a sufficient amount of quality samples. An expanded dataset can improve the quality of the predictions [cite Ying 2019], however only when the model has already performed well on the initial dataset. If the model was performing badly initially, adding more data will not solve the problem. Here having TODO n samples of data the model was trained on TODO samples only to find the best structure and regularization first, afterwards the model was retrained using more data and the loss imroves from 0., to TODO.
\end{itemize}
