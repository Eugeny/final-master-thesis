All regularization techniques apart from the network reduction mentioned in Section \ref{section:regularization-theory} were used in this work. For example, in ER training (see Figure \ref{fig:er-overfit}) one can clearly observe overfitting, that was mitigated with the use of weight decay and augmentations. Batch normalization layers were added to the UNet architecture not only for regularization purposes, but for an imroved speed of learning as well. Network reduction was not used in this case as it was proven that bigger model captures finer details in a better way (see Figure \ref{fig:better-nuclei}). Finally, expansion of the data has a strong impact on traning: having $27,264$ crops of data the model was trained on $5,376$ crops only (two 96-well plates) to find the best structure and regularization first, afterwards the model was retrained using more data and the PCC loss improves from $0.77$, to $0.93$.