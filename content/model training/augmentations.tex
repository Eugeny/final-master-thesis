Augmentations are a powerful regularization technique that helps the network to generalize better (\cite{Perez_Wang_2017}) and is an effective solution for a situation with the lack of labeled data (\cite{Yang_2022}). The main idea behind augmentation is to increase the diversity of training data, when the acquisition of the real training data is expensive. Augmenting existing images creates the new synthetic samples which are hypothetically very close to the original true distribution of images. However, one should be very cautious regarding the type of augmentations that can be used. Augmentation must enrich the dataset, but it should not change the semantics hidden in each image. The peculiarity of current research is the pair-wise correspondence between the DIC input and the output fluorescence. One should not change the input in such a way that the fluorescence signal could not be inferred from it. Therefore the following augmentations have been used on cropped images:

\begin{itemize}
	\item \textbf{Flipping}: Flip the crop horizontally (30\% chance), vertically (30\% chance) or both.
	\item \textbf{Random rotation}: Rotates the crop by a random angle.
	\item \textbf{Random scaling}: Reduces crop size by 100 (20\% chance), 50 (20\% chance) or 20 (60\% chance) pixels from each side (down, top, left, right).
	\item \textbf{Contrast}: Halves an image's constrast (50\% chance), or triples it (50\% chance). Applied with a 20\% chance.
	\item \textbf{Defocus blur}: Imitates a defocus blur of severity level 4 (see section \ref{section:defocus-blur}) on an image with a 20\% chance.
\end{itemize}

\paragraph{Special augmentations for rotation and scaling}

Augmentations are chosen to be applied during training and the reason for that is to preserve the same conditions regarding the size of the datasets in order to have a fair comparison between approaches. For example, one decides to augment 20\% of images and add them to an original dataset in order to expand it. Then the comparison between training with and without augmentations would be unfair as the sizes of the datasets are different, and it is not clear whether the performance improvement or decrease comes from the longer training (enlarged dataset) or augmentations.

Since augmentations are applied on crops directly during training there is a possibility to improve the rotation and scale of crops. The problem with these augmentations are depicted in Figure \ref{fig:smart-augments} --- after applying them the a gray background will appear which would be filled with zeros in PyToch implementation. However, since an original image where the crop has been cut out is available, one can easily restore the background with the original values and avoid this problem entirely.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.7\linewidth]{bilder/model training/smart augmentations.png}
		\caption{Improving augmentation by using original image for rotation and scaling}\label{fig:smart-augments}
	\end{center}
\end{figure}

Applying augmentations in combination with other regularization techniques has proven to be helpful for nuclei predictions and against overfitting for ER training (see Figure \ref{fig:nuclei-comparison-predictions}, see Figure \ref{fig:er-overfit}). Therefore it is recommended to use it in further research as well.
