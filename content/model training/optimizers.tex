It is also important to choose a correct optimizer in order for a model to converge as fast as possible. Here, three different optimizers have been tried out --- namely, SGD, Adam and Adadelta optimizers. As a result, the SGD optimizer performed the worst, while Adam and Adadelta optimizer performed similarly with Adadelta converging to slightly better values in the end. Adam optimizer has required some fine-tuning of the learning rate from 0.001 to 0.0001 to achieve the best result. Both Adadelta and Adam can be used for model optimization in this dataset. The experiments were conducted on the truncated dataset of nuclei images using PCC loss.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.8\linewidth]{bilder/model training/optimizer-comparison.png}
		\caption{Comparison of convergence for different optimizers}\label{fig:optimizers}
	\end{center}
\end{figure}
