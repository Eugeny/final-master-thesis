This reseach additionally provides the study of the embeddings of a trained UNet and an Autoencoder in Chapter [TODO cite chapter]. In order to understand the visualizations better all dimensionality reduction methods that were used here are listed and explained in this subsection.

\begin{definition}[Embedding]
    An embedding in this context is an output tensor from the encoder part of the UNet or from an encoder part of an Autoencoder.
\end{definition}

The encoder output of the UNet is a tensor of size $16 \times 16 \times 256$ and after its flattening it turns into a vector of size $655536$. The smallest autoencoder embedding was of size 200 [TODO check] which is also high-dimensional. One of the tasks of this research if to determine wethere there are any interesting patterns or grouping based of various criteria hidden within the bottleneck embeddings, and wether they could be useful for further research. Yet in order for humans to comprehend the embeddings we need to maps them either to 2D or 3D vectors and that is where dimensionaluty reduction algorithms are essential.

\paragraph{UMAP}
Dimension reduction algorithms mostly form two main categories: ones are stronger preserving the pairwise
distance globally - meaning try to preserve the structure amongst all the data samples; others prefer to save local distances. For example PCA [cite Hotelling] are assigned to the first caterogy, while t-SNE [cite Ulyanov] and Isomap are assinged to a latter one.

Uniform Manifold Approximation and Projection (UMAP) was build in a way to preserve both and it is a competitor of t-SNE approach, however is much faster and provides a transformation that can be used on the new data. UMAP is a graph-based algorithm and uses a k-nearest graph as a foundation. As any graph-based algorithm, its structure also includes two main steps: 

%[TODO need a synonym instead of ambient == surround]
\begin{itemize}
    \item Graph construction procedure. During this stage a weighted k-neighbour graph will be constructed from the data. Specific transformation are applied on its edges to surround local distance. And the strong asymmetry common to k-neighbour graphs will be reduced.
    \item Graph layout building. In this stage one needs first to define an objective function that can preserve desired graph characteristics and then find a low dimensional representation of the graph that will minimize the objective.
\end{itemize}

%https://pair-code.github.io/understanding-umap/
In short, UMAP optimizes a low-dimensional graph from the high-dimensional one to be structurally very similar to each other. The algorithm has to important hyperparameters, which should be chosen carefully: \textit{n\_neightbors} and \textit{min\_dist}. The first one balances the local versus the global structure of the graphs, the higher the values the more finer details will be lost. The latter one controls how densly points will be located to one another. Higher values of this parameter results in a loosier structure that preserves a broader topoly of the data. 

\paragraph{PacMAP}
Pairwise Controlled Manifold Approximation (PacMAP) is another dimensionality reduction method that is able to preserve both local and global data structure in a lower dimension space. Unlike other methods that regulate the stronger preservance of global structure by using more neightbors, PaCMAP uses mid-near pairs, to first capture global structure and then refine local structure, which both preserve global and local structure. It introduces a the following parameters: neighbor pairs (pair\_neighbors), mid-near pair (pair\_MN), and further pairs (pair\_FP). [cite Yingfan] The neighbor pairs parameter is used during the building of the k-Nearest Neighbor graph. It is recommended to use the value around 10 for datasets with size smaller than 10000 [cite their repo]. The mid-near pair ratio parameter is the ratio of the number of mid-near pairs to the number of neighbors, whereas further pairs ratio is the the ratio of the number of further pairs to the number of neighbors. Configuring these parameters allow the user to achieve the desired ration between preserving local and global structure. Such method also works faster than UMAP, which allows to try out more hyperparameter options.

\paragraph{PCA}
Principal component analysis (PCA) is an algorithm for linear dimensionality reduction [cite Pearson 1901].  PCA maximizes the variance in data's low-dimensional representation in order to keep as much information
as possible. Essentially PCA gives projections $\tilde{x^{(i)}}$ for input samples $x^{(i)}$ that would be very similar to them, however have a much smaller dimensionality. Eigenvectors of the data covariance matrix are the directions of the most variance within the data, and the eignevalues correspoding to them are the amount of variance hidden in each dimension. Therefore by projecting the data using the eigenvectors with the largest eingenvalues one will preserve the most varience of the data possible [cite MML-book].

The steps of PCA algorithm are the following:
\begin{itemize}
    \item Subtract mean $\mu_d$. To center the input data is not a nessesary step, but it is recommended to do so to avoid the numerical problems.
    \item Standartize the data. Calculate the standard deviation $\sigma_d$ and standartize the data to have unit variance for every dimension. 
    \item Do an eigendecomposition of the data covariance matrix. For that one first must compute the coveriance matrix itself, since the covariance matrix is symmetric from the spectral theorem one can always find an orthonomal basis of eigenvectors. 
    \item Project the data. For that first standartize the point $x^* \in \mathbb{R}$ using $\mu_d$ and $\sigma_d$:
        \begin{equation}
            x^*_d  \leftarrow \frac{x^*_d - \mu_d}{\sigma_d}
        \end{equation}
        where $d = 1, ..., D$ and $x^*_d$ is a $d$-th component of vector $x^* \in \mathbb{R}^D$
    Get the projection as 
    \begin{equation}
        \tilde{x^*} = BB^T x^*
    \end{equation}
    with coordinates $z^* = B^Tx^*$. Here $B$ is a matrix of eigenvectors associated with the biggest eigenvalues of a covariance matrix.
\end{itemize}
[cite MML-book]