This research also provides a study of the embeddings of a trained UNet and an autoencoder in Chapter \ref{section:unet-embeddings-study}. In order to better understand the visualizations, all dimensionality reduction methods that were used are listed and explained in this subsection.

\begin{definition}[Embedding]
    An embedding in this context is an output tensor from the encoder part of the UNet or from an encoder part of an autoencoder.
\end{definition}

The encoder output of the UNet is a tensor of size $16 \times 16 \times 256$ and after its flattening it turns into a vector of dimensionality $655536$. The smallest autoencoder embedding was of size 200 which is also high-dimensional. One of the tasks of this research is to determine whether there are any interesting patterns or grouping based on various criteria hidden within the bottleneck embeddings, and whether they could be useful for further research. Yet in order for humans to comprehend the embeddings we need to map them either to 2D or 3D vectors. In this context dimensionality reduction algorithms are essential.

Dimension reduction algorithms mostly form two main categories: ones are stronger preserving the pairwise
distance globally --- meaning they are trying to preserve the structure among all the data samples; others prefer to save local distances. For example, PCA (\cite{Hotelling_1933}) are assigned to the first caterogy, while t-SNE (\cite{tsne}) and Isomap are assinged to the latter one.

\paragraph{PCA}
Principal component analysis (PCA) is an algorithm for linear dimensionality reduction (\cite{Pearson_1901}).  PCA maximizes the variance in data's low-dimensional representation in order to keep as much information as possible. Essentially, PCA gives projections $\tilde{x^{(i)}}$ for input samples $x^{(i)}$ that would be very similar to them, however have a much smaller dimensionality. Eigenvectors of the data covariance matrix are the directions of the most variance within the data, and the eigenvalues correspoding to them are the amount of variance hidden in each dimension. That is why by projecting the data using the eigenvectors with the largest eigenvalues, one will preserve the most varience of the data possible (\cite{mml_book}).

The steps of PCA algorithm are the following (\cite{mml_book}):
\begin{itemize}
    \item Subtract mean $\mu_d$. Centering the input data is not a necessary step, but it is recommended to do so to avoid numerical problems.
    \item Standardize the data. Calculate the standard deviation $\sigma_d$ and standardize the data to have unit variance for every dimension. 
    \item Do an eigendecomposition of the data covariance matrix. To do so one must first compute the covariance matrix itself, since the covariance matrix is symmetric from the spectral theorem one can always find an orthonormal basis of eigenvectors. 
    \item Project the data. First, standardize the point $x^* \in \mathbb{R}$ using $\mu_d$ and $\sigma_d$:
        \begin{equation}
            x^*_d  \leftarrow \frac{x^*_d - \mu_d}{\sigma_d}
        \end{equation}
        where $d = 1, ..., D$ and $x^*_d$ is a $d$-th component of vector $x^* \in \mathbb{R}^D$
    Get the projection as 
    \begin{equation}
        \tilde{x^*} = BB^T x^*
    \end{equation}
    with coordinates $z^* = B^Tx^*$. Here $B$ is a matrix of eigenvectors associated with the biggest eigenvalues of a covariance matrix.
\end{itemize}

\paragraph{Uniform Manifold Approximation and Projection (UMAP)}
UMAP (cite \cite{McInnes_2020}) was built in a way to preserve both and it is a competitor of the t-SNE approach. However, it is much faster and provides a transformation that can be used on the new data. UMAP is a graph-based algorithm and uses a k-nearest graph as its foundation. As with any graph-based algorithm, its structure also includes two main steps: 

%[TODO need a synonym instead of ambient == surround]
\begin{itemize}
    \item Graph construction procedure. During this stage a weighted k-neighbour graph will be constructed from the data. Specific transformations are applied on its edges to surround local distance. And the strong asymmetry common to k-neighbour graphs will be reduced.
    \item Graph layout building. In this stage one first needs to define an objective function that can preserve the desired graph characteristics and then find a low dimensional representation of the graph that will minimize the objective.
\end{itemize}

%https://pair-code.github.io/understanding-umap/
In short, UMAP optimizes a low-dimensional graph from the high-dimensional one to be structurally very similar to each other. The algorithm has two important hyperparameters, which need to be chosen carefully: number of neighbors (\textit{n\_neighbors} in code) and minimum distance (\textit{min\_dist} in code). The first one balances the local versus the global structure of the graphs; the higher the values the more fine details will be lost. The latter one controls how densely points will be located to one another. Higher values of this parameter result in a looser structure that preserves a broader topology of the data (\cite{umap_web}).

\paragraph{PaCMAP}\label{section:pacmap}
Pairwise Controlled Manifold Approximation (PaCMAP) is another dimensionality reduction method that is able to preserve both local and global data structure in a lower dimensional space. Unlike other methods that regulate the stronger preservance of global structure by using more neighbors, PaCMAP uses mid-near pairs to first capture global structure and then refine local structure, which both preserve global and local structure. It introduces the following parameters: neighbor pairs (\textit{n\_neighbors}), ratio of mid-near pairs to nearest neighbor pairs (\textit{MN\_ratio}), and ratio of further pairs to nearest neighbor pairs (\textit{FP\_ratio}) (\cite{Wang_2021}). The neighbor pairs parameter is used during the building of the k-nearest neighbor graph. It is recommended to use a value of around $10$ for datasets with a size smaller than $10000$ (\cite{Yingfan_git}). Configuring these parameters allows the user to achieve the desired ratio between preserving local and global structure. This method also has a faster runtime than UMAP, which allows to try out more hyperparameter options. For more details on how this methods works refer to \cite{Wang_2021}.
